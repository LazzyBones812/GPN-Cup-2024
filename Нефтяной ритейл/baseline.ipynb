{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подключение библиотек и чтение данныx**\n",
    "\n",
    "- *Токенизатор взят из библиотеки ***yargy*** *\n",
    "- *Стоп-слова на русском взяты из ***nltk*** *\n",
    "- *Регулярные выражения для очистки текста от знаков препинания и прочего мусора - библиотека ***re*** *\n",
    "- *Для работы в датафреймом - ***pandas*** *\n",
    "- *Метод преобразования слов в вектор, алгоритм LDA, KMeans и алгоритм уменьшения размерности PCA - ***scikit-learn*** *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yargy.tokenizer import MorphTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_json('data/cintra_phoenix_oils_hr_mgck_feather.json').set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Определение токенизатора и множества стоп-слов на русском**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MorphTokenizer()\n",
    "ru_stopwords = set(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В исходных текстах много различных знаков препринания, эмодзи и прочего \"мусора\". Проведем очистку данных - оставляем только слова и/или цифры, а также переводим слова в нижний регистр**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df['quote'].apply(lambda x: re.findall(r'\\b\\w+\\b', str(x).lower())).to_frame()\n",
    "df = df['quote'].apply(lambda x: ' '.join(x)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Формирование токенов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tok_2 = [[] for _ in range(len(df.quote.values))]\n",
    "for i in range(len(df.quote.values)):\n",
    "    list_tok = []\n",
    "    tok = tokenizer(df.quote[i])\n",
    "    for t in tok:\n",
    "        if t.normalized not in ru_stopwords:\n",
    "            list_tok.append(t.normalized)\n",
    "    list_tok_2[i] = ' '.join(list_tok)\n",
    "df['quote'] = list_tok_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Векторизация текста. Используется TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.6, min_df=5)\n",
    "X = vectorizer.fit_transform([text for text in df.quote.values]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Уменьшение размерности**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выделение кластеров. Количество кластеров - это гиперпараметр, в решении формируется пять кластеров**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "list_cluster = []\n",
    "for cluster_idx in range(num_clusters):\n",
    "    cluster_documents = [df.quote.values[i] for i, label in enumerate(kmeans.labels_) if label == cluster_idx]\n",
    "    list_cluster.append(cluster_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Использование алгоритма LDA для анализа полученных кластеров. Устанавливается количество кластеров равных единице, так как наш текст уже разбит на кластеры и нам нужно уже к текстам в кластере найти те слова, которые полно описывают рассматриваемый кластер**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема для кластера № 1: форма, делать, колдун, стать, квартал, качество, слышать, свой, огонь, давать\n",
      "Тема для кластера № 2: цк, огненный, стать, день, каждый, феникс, зачаровать, приходиться, зарплата, делать\n",
      "Тема для кластера № 3: колонка, бандит, далёкий, равно, помочь, случиться, замечательный, пара, супер, сотрудник\n",
      "Тема для кластера № 4: чан, будуть, расчитывать, чен, арений, ууйти, предложить, зпл, зна, телепортировать\n",
      "Тема для кластера № 5: друг, ребёнок, информация, отдельный, прийти, нужный, добираться, кофе, зрыть, вериться\n"
     ]
    }
   ],
   "source": [
    "num_topics = 1\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "\n",
    "for j in range(len(list_cluster)):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.07, min_df=1)\n",
    "    X = vectorizer.fit_transform([text for text in list_cluster[j]]).toarray()\n",
    "    lda.fit(X)\n",
    "    for topic_idx, topic_words in enumerate(lda.components_):\n",
    "        top_words_idx = topic_words.argsort()[-10:][::-1]\n",
    "        top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]\n",
    "        print(f\"Тема для кластера № {j + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В результате формируются определенное количество тем и список слов, которые описывают эти темы**\n",
    "\n",
    "Настраиваемые параметры \n",
    " - *max_df* - параметр векторизатора, процент самых частоповторяющихся слов, которые удаляются\n",
    " - *min_df* - параметр векторизатора, порог удаления самых непопулярных слов\n",
    " - *num_clusters* - параметр кластеризатора KMeans, количество кластеров\n",
    "\n",
    "max_df в последнем vectorizer такой низкий, так как в датасете много слов, которые не несут важной информации. \n",
    "\n",
    "В итоге получилось пять кластеров\n",
    " 1. Что-то про качество, форму, а также про временные промежутки (год, квартал). Похоже форму приходится менять каждый год или квартал, так как у нее плохое качество \n",
    " 2. Фигурируют слова \"условие\", \"дракон\", \"поход\". Так как в названиях фентезийных компаний часто содержится слово \"Дракон\", видимо текста описывают условия работы в компаниях. \n",
    " 3. Фигурируют слова \"бандит\", \"помочь\", \"случиться\", \"сотрудник\". Похоже бандиты представляют серьезную опасность для сотрудников\n",
    " 4. Фигурируют слова \"зарплата\", \"дело\", \"ночь\", \"час\". Похоже речь идет про ночные смены, которые оплачиваются каждый час\n",
    " 5. Много раз фигурировали слова \"ребенок\" и \"друг\". Также встречались слова \"добираться\", \"прийти\", \"нужный\", \"информация\". Возможно речь про нужную информацию, которую нужно срочно передать кому-то хорошо знакомому. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
